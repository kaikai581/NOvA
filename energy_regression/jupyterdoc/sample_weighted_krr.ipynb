{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample-Weighted Kernel Ridge Regression for Neutrino Energy Reconstruction\n",
    "## Motivation\n",
    "In the past few years, we have seen the application of state-of-the art machine learning techniques, primarily the convolutional neural network (CNN), to do particle identification (PID) in our experiment. What phycisists call PID belongs to the classification task in machine learning. We have seen tremendous improvements in all metrics with this technique over the former more traditional ways, such as a log-likelihood based method.\n",
    "\n",
    "Equally important is our events' energy reconstruction, corresponding to machine learning's regression task. Curiously enough, we are still using the relatively simpler techniques to model our nonlinear energy response. In particular, we use the spline fit, basically our home-brew [MARS](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines) model. This is not saying we have no intention to apply the more advanced regression techniques to this problem. Actually, we do have a group of collegues applying the same network structure the PID uses, but adding one regression layer, to reconstruct the energy. It is still under intense investigation at the time of writing.\n",
    "\n",
    "Months ago I happened to grab a [machine learning textbook](https://www.amazon.com/Machine-Learning-Optimization-Perspective-Developers/dp/0128015225/ref=sr_1_36?ie=UTF8&qid=1522357862&sr=8-36&keywords=machine+learning), and found a class of techniques called kernel methods to have very beautiful mathematical structure. I became fascinated with those techniques, and started to have ideas to try them out on our data and see if that would improve our energy estimation.\n",
    "\n",
    "The answer is **yes**. Now, let me walk you through.\n",
    "## Ridge Regression\n",
    "Let's start with the most basic. Given $N$ training samples $(\\mathbf{x}_i,y_i)$, where $\\mathbf{x}_i$'s $\\in\\mathbb{R}^\\ell$ are the regressors and $y_i$'s $\\in\\mathbb{R}$ are the targets, we want to find a linear function in $\\mathbf{w}$ $$f_{\\mathbf{w}}(\\mathbf{x})=\\mathbf{w}^T\\mathbf{x}$$ that minimizes the square error cost function $$C(\\mathbf{w})=\\frac{1}{2}\\sum_{i=1}^{N}(y_i-\\mathbf{w}^T\\mathbf{x}_i)^2$$\n",
    "By differentiation with respect to $\\mathbf{w}$, it is equivalent to solving the **normal equation**,\n",
    "$$\\mathbf{X}^T\\mathbf{X}\\mathbf{w}=\\mathbf{X}^T\\mathbf{y}$$\n",
    ", where $\\mathbf{X}$ is the so called _design matrix_, $$\\mathbf{X}=\\begin{pmatrix}\\mathbf{x}_1^T \\\\ \\vdots \\\\ \\mathbf{x}_N^T \\end{pmatrix}$$\n",
    "If $\\mathbf{X}^T\\mathbf{X}$ is invertible, $\\mathbf{w}$ is readily solved. However, in practice, the predictor variables are very often nearly linearly dependent, leading to a phenomenon called [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity). In such cases, the resulting $\\mathbf{w}$ becomes highly sensitive to variations in training samples, leading to overfitting. One of the most popular remedies is the [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization): Instead of minimizing the above cost function, minimize this one $$C(\\mathbf{w})=\\frac{1}{2}\\sum_{i=1}^{N}(y_i-\\mathbf{w}^T\\mathbf{x}_i)^2+\\frac{1}{2}\\alpha\\Vert\\mathbf{w}\\Vert^2$$, resulting in $\\mathbf{w}=(\\mathbf{X}^T\\mathbf{X}+\\alpha\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}$. In regression jargon this is called _ridge regression_.\n",
    "## Dual Formulation\n",
    "Since\n",
    "\\begin{align}\n",
    "  \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T+\\alpha\\mathbf{I})&=(\\mathbf{X}^T\\mathbf{X}+\\alpha\\mathbf{I})\\mathbf{X}^T \\\\\n",
    "  \\Rightarrow \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T+\\alpha\\mathbf{I})^{-1}&=(\\mathbf{X}^T\\mathbf{X}+\\alpha\\mathbf{I})^{-1}\\mathbf{X}^T \\\\\n",
    "  \\Rightarrow \\mathbf{w}=(\\mathbf{X}^T\\mathbf{X}+\\alpha\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}&=\\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T+\\alpha\\mathbf{I})^{-1}\\mathbf{y}\n",
    "\\end{align}\n",
    "Given a test sample $\\hat{\\mathbf{x}}$, its predicted value is $\\hat{y}=\\hat{\\mathbf{x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
