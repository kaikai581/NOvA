%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 2.0 (10/06/16)
%
% Attention:
% 0. The self-defined font is used, because 'Calibri' is 
% not supported in the latex font packages. 'LuaLatex'
% should be used.
% 1. This template has been generated according to 
% the Power Point template of LUMC in 2016. 
% 2. This is generated purely with images as the 
% background.
% 3. The bullet point color was used purely for personal 
% preference. 
% 4. Any more adding to the template are welcome. 
% 5.In order to use the navigation bar, the title 
% for each section should not be to long. 
% 6.Adding animation is possible. I prefer to add another
% pdf file with:
% \animategraphics[parameters]{1}{fname}{startnum}{endnum}
% 7. This is my first template, the files might be not  
% well organized, sorry for that. 
% 
% Author:
% Shengnan Liu
% sliu729@gmail.com
% Division Medical imaging processing, 
% Leiden University Medical Center
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Modification Log:
% Generated by Shengnan Liu on 21-01-2016
% Cleaned up for further usage on 10-06-2016
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[aspectratio=169]{beamer}
\beamertemplatenavigationsymbolsempty%
\usepackage{textpos}
\usefonttheme[onlymath]{serif}
\usepackage{amsmath}
\usepackage{eso-pic}
\usepackage{fontspec}
\usepackage{textcomp}
\usepackage{multirow}
\setsansfont{Linux Biolinum O}
%\setmainfont{Gentium}
\usetheme{Rochester}% theme
\usecolortheme{seahorse}
\colorlet{beamer@blendedblue}{green!40!black}
%\setbeamertemplate{navigation symbols}{}
%\setbeamertemplate{footline}[frame number]
\setbeamertemplate{footline}{% hide total number of slides
  \hfill% 
  \usebeamercolor[fg]{page number in head/foot}% 
  \usebeamerfont{page number in head/foot}% 
  \insertframenumber%
  \kern1em\vskip2pt% 
}
\setbeamertemplate{itemize items}[circle]
\hypersetup{
    colorlinks = true,
    urlcolor = cyan
}

\addtobeamertemplate{frametitle}{}{%
\begin{textblock*}{100mm}(.80\textwidth,-1.55cm)
\includegraphics[height=1.5cm]{figures/csu-logo.png}
\end{textblock*}}

\title{A Shallow Learning Hadronic Energy Estimator}
\date[today]{\today}
\author{Shih-Kai Lin}
\institute{Colorado State University}
\usepackage{multicol} % 
%\usepackage{animate}  % animation
\usepackage{amsmath,amsfonts,amssymb} % This makes the equations appears better 
\begin{document}
%The title
\begin{frame}
\titlepage
\end{frame}


% Adding note that cannot be seen
\note[itemize]{
\item Good morning everyone. Today I would like to present the work that were submitted to SPIE proceeding photonics west. 
\item point 2
}

% The outline
%\begin{frame}{Outline}
%\tableofcontents
%\end{frame}

\begin{frame}{Some Teaser}
  \centering
  \includegraphics[width=\textwidth]{figures/ehad_spec_res_side_by_side.pdf}
\end{frame}

\begin{frame}{Motivation}
  \begin{itemize}
    \item NOvA has put a lot of effort into PID (classification) with the state-of-the-art machine learning techniques, but not as much in energy reconstruction (regression).
    \begin{itemize}
      \item Except CVN regression (UCI)
    \end{itemize}
    \item Why one more attempt at energy reconstruction besides the current prong-based one (Erica, Michael) and CVN regression?
    \begin{itemize}
      \item It is a natural generalization to the current official spline fit.
      \begin{itemize}
        \item In the sense that it also uses event-level variables to fit a regression function.
      \end{itemize}
      \item It has welcoming mathematical properties and beautiful underlying theory.
      \item The nice mathematical properties are reflected in the results.
      \item Better tools! There are many CVN final state particle scores available at the moment.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Shallow Learning}
  \begin{itemize}
    \item As opposed to deep learning. Some authors use this term in literature.
    \begin{itemize}
      \item I personally like it due to my initials...
    \end{itemize}
    \item Below is why this class of methods is called shallow learning in contrast to deep learning:    
  \end{itemize}
  \centering
  \scriptsize
  \begin{tabular}{c|ccccc}
    \multirow{2}{*}{deep architecture} & \multirow{2}{*}{CNN} & \multirow{2}{*}{$\longrightarrow$} & \multirow{2}{*}{many hidden layers} & \multirow{2}{*}{$\longrightarrow$} & classification    \\
    &&&&&regression\\
    \hline
    \multirow{2}{*}{shallow architecture} & support vector machine & \multirow{2}{*}{$\longrightarrow$} & one hidden layer & \multirow{2}{*}{$\longrightarrow$} & classification\\
    & kernel ridge regression & & (feature map) & & regression\\
  \end{tabular}
  \normalsize
  \begin{itemize}
    \item A cohort of \emph{kernel methods} belongs to shallow architecture, among which the support vector machine was so popular that it almost killed neural network in the early 2000 before CNN took the crown.
    \item I will quickly go through the ideas behind kernel methods to justify the use of them for an energy estimator.
  \end{itemize}
\end{frame}

\begin{frame}{Ideas behind Kernel Methods -- from the Most Basic}
\scriptsize
Linear regression:\\
Given $N$ training samples, $(\mathbf{x}_i,y_i)$, $i=1,...,N$, where $\mathbf{x}_i$'s $\in\mathbb{R}^\ell$ are predictor variables and $y_i$'s $\in\mathbb{R}$ are target variables of training samples, find a linear function
\begin{equation}
  f_\mathbf{w}(\mathbf{x})=\mathbf{w}^T\mathbf{x}
\end{equation}
that minimizes the quadratic cost,
\begin{equation}
  C(\mathbf{w})=\frac{1}{2}\sum_{i=1}^{N}(y_i-\mathbf{w}^T\mathbf{x}_i)^2
\end{equation}
$\mathbf{w}$ that minimizes the cost function is readily found by solving the \emph{normal equation}:
\begin{equation}
  \mathbf{X}^T\mathbf{X}\mathbf{w}=\mathbf{X}^T\mathbf{y}
\end{equation}
, where $\mathbf{X}$ is the so called \emph{design matrix}, 
\begin{equation}
  \mathbf{X}=\begin{pmatrix}\mathbf{x}_1^T \\ \vdots \\ \mathbf{x}_N^T \end{pmatrix}
\end{equation}
\end{frame}

\begin{frame}{Ridge Regression}
\scriptsize
Very often, the predictor variables vary in a similar way, known as near collinearity. In such cases, $\mathbf{X}^T\mathbf{X}$ is almost singular, and the resulting $\mathbf{w}$ becomes highly sensitive to variations, leading to overfitting.\\
Applying Tikhonov regularization leads to ridge regression, namely, minimizing the cost function
\begin{equation}
  C(\mathbf{w})=\frac{1}{2}\sum_{i=1}^{N}(y_i-\mathbf{w}^T\mathbf{x}_i)^2+\frac{1}{2}\alpha\Vert\mathbf{w}\Vert^2
\end{equation}
with the solution
\begin{equation}
  \mathbf{w}=(\mathbf{X}^T\mathbf{X}+\alpha\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}
The cost function is convex, which guarantees a global minimum. (Very different for NN case.)\\
Note that $\mathbf{w}$ can be rewritten\footnote{See \href{http://stat.wikia.com/wiki/Kernel_Ridge_Regression}{here} for details.} as $\mathbf{w}=\mathbf{X}^T(\mathbf{X}\mathbf{X}^T+\alpha\mathbf{I})^{-1}\mathbf{y}$.\\
For a test sample $\hat{\mathbf{x}}$, the predicted value $\hat{y}=\mathbf{w}^T\hat{\mathbf{x}}=\hat{\mathbf{x}}^T\mathbf{w}$
\end{frame}

\end{document}